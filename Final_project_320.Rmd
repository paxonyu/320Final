---
title: "Google Play Store App Data and How To Use It"
output: html_document
knit: (function(input_file, encoding) {
  out_dir <- 'Documents';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
---

##Data Acquisition
Welcome ladies and gentlemen, to the tutorial that will make the rest of your life feel underwhelming. In today's tutorial, we will be looking at google play store data, how to acquire it, how to parse it, and the many applications we can get from it. To start off, we're going to need the actual data. I'm using a csv file which stands for "comma separated file" and that is simply a file structure that contains my data. If you want to follow along you can get the same data from [here](https://www.kaggle.com/lava18/google-play-store-apps/downloads/google-play-store-apps.zip/6) 

```{r setup, message=FALSE,warning=FALSE}
library(tidyverse)
library(ggplot2)
library(broom)
main_data <- read_csv("Documents/googleplaystore.csv")
```

Once we've stored the data in a variable name, we're going to view the data to see what we are working with. I've used the head and tail method in order to view both ends of my data.

```{r viewing the data}
head(main_data)
tail(main_data)
```

#Data Cleaning And Parsing

We can see that we have a lot categories from our data. In order, we have our App (our title), the category the app belongs to, the current rating, the number of reviews, the size of the app, the number of installs, whether it is a paid or free app, the price (0 if free), the content rating, the genre the app falls into, the last updated date, the current version of the app and the android version of the app.
Right off the bat, we can see that we're going to run into some trouble with the current data. For example, if we wanted to analyze how many installs an app has, we're going to need the installs column to function as a number. However, currently the Installs column functions as a series of characters because of the "+" appended to the end of it. Let's convert the column into a double value by first removing the "+" and the commas at the end of and within each entry and converting the type of the column into a double.
```{r column_type_change}
main_data$Installs = gsub("\\+","",main_data$Installs)
main_data$Installs = as.numeric(gsub("\\,","",main_data$Installs))
head(main_data)
```
As we can see, this worked out very well and we now have an a numeric column for us to work with when analyzing the data. One thing to mention is that data set that we find outselves using does not have exact numbers on installs which means that any analysis will have some sort of error. For example, if we had 10,000+, the app could have any value between 10,000 and 10,999 assuming that the next value would have been 11,000+. There is nothing that we can do about this since the dataset does not give us the exact values. 
Before continuing, I will also make a few changes to the columns such as making the price column a numeric value and I will convert the Last Updated column into a date time variable. The last thing I will do is remove the genres column. While some apps can belong in other categories, the genre's column for the most part is a repeat of the category column. In addition, I will also modify the size column. I will truncate the "M" and the assumed measuring units will be megabytes. Size also has an interesting quality. 
Some entries have a "Varies with Device" when it comes to size most likely due to hardware limitations. Since no app can have a size 0, I will be replacing all of the "Varies with Device" entries with 0. In addition, some less popular apps have no rating, I will have the default rating set to 0.
```{r more_column_changes}
library(readr)
main_data$Price = gsub("\\.","",main_data$Price)
main_data$Price = as.numeric(gsub("\\$","",main_data$Price))
main_data <- main_data %>%
  type_convert(col_types=cols(`Last Updated`=col_datetime(format="%B %d, %Y")))
main_data$Genres = NULL
main_data$Size = gsub("M","",main_data$Size)
main_data$Size = as.numeric(gsub("Varies with device","0",main_data$Size))
main_data %>%
  replace_na(list(Rating=0))
main_data
```

#Exploratory Data Analysis

Now that our data is fully prepared, it's time to see what information we can collect. For the purposes of this tutorial, let's go over some basic functions in R. In R, we have a method called filter which will filter out entries based on a condition.
So, for example, we want to find the really popular apps and our definition of popular is an app that gains more than 1,000,000 downloads. Well, with our already clean data, we can acquire that information like this. In addition, let's make a modification in our main table to identify which apps are popular.
```{r popular}
main_data <- main_data %>%
  mutate(is_popular=ifelse(Installs < 1000000,FALSE,TRUE))
popular_apps <- filter(main_data,Installs > 1000000)
popular_apps
```
Wow, truly an incredible feat of technology. However, we can go one step further and try and see some summary stats. We can do this by grouping these apps by their category. So for this example, to keep the numbers simple, we'll be using the table we just created of all the popular apps. Let's see, we'll do total number of installs, total number of reviews, ratio of installs to reviews, and average rating. We're doing everything so try and keep up.
```{r everything}
popular_apps %>%
  group_by(Category) %>%
  summarize(total_installs=sum(Installs),total_review=sum(Reviews),ratio_install_to_review=total_installs/total_review,average_rating=mean(Rating)) %>%
  arrange(desc(total_installs))
```
A lot of juicy information in the table above all depending on what you sort it by. In our case, we arranged the inforamtion in descending order based on total installs. To nobody's surprise, the game category has an explosive lead in terms of total_installs. Other cool things to note is that the average rating for every category hovers around 4.2/4.3 range. This is probably because our sample size is just the most popular apps as opposed to our entire data set. 

Another useful method is the select method which allows us to select particular attributes from tables. For example, if we wanted to streamline our data processing by only looking at the attributes that we will be using, we can use the select function.

```{r useful_att}
useful_att <- main_data %>%
  select("App","Installs","Category","Last Updated","Size")
useful_att
```
With these tools, we can gather important information like summary statistics by category.
```{r most_pop}
most_pop <- main_data %>%
  group_by(Category) %>%
  summarize(min=min(Installs),max=max(Installs),mean=mean(Installs)) %>%
  arrange(desc(mean))
most_pop
```
For later, let's get the top 10 most popular categories by average installs.

```{r top_ten_cate}
most_pop <- main_data %>%
  group_by(Category) %>%
  summarize(min=min(Installs),max=max(Installs),mean=mean(Installs)) %>%
  arrange(desc(mean)) %>%
  slice(1:10)
most_pop
```


This gives me a wonderful idea to compare the aggregate data to the data of our most popular apps. Let's get started.

The language R has this limitation that when we're dealing with large numbers, if the numbers get too large, the program tends to spit out NaN. To combat this, for our comparison exercises we'll be using two data sets. One data set will consist of the top 100 apps (based on installs) and the other will consist of the lowest 100 apps.

```{r unpopular, warning=TRUE}
unpopular_apps <- main_data %>%
  anti_join(popular_apps, by=c("App","Reviews","Installs"))  


popular_app_stats <- popular_apps %>%
  summarize(total_installs=sum(Installs),total_review=sum(Reviews),ratio_install_to_review=total_installs/total_review,average_rating=mean(Rating),mean_installs=mean(Installs)) 
popular_app_stats

unpopular_app_stats <- unpopular_apps %>%
  summarize(total_installs=sum(Installs,na.rm=TRUE),total_review=sum(Reviews,na.rm=TRUE),ratio_install_to_review=total_installs/total_review,average_rating=mean(Rating,na.rm=TRUE),mean_installs=mean(Installs,na.rm=TRUE)) 
unpopular_app_stats

combined = rbind(popular_app_stats,unpopular_app_stats)
combined
```

We've done an anti join which is to say that unpopular_apps will filter out everything that ISN'T in popular_apps. The resulting data shouldn't really be surprising to anybody. The total number of installs is higher in the popular app group by definition as is the number of mean installs. However, more interestingly, popularity does not seem to determine the install to review ratio nor does it seem to affect average rating a lot. I would've expected to see more variance with the average rating but this is a summary statistic after all.

## Graphing Relationships

With the power of R, we can do anything graph related. We can even plot the relationship between any given variables. Let's start with something simple. We can start with the relationship between Installs and Size.

```{r Installs_Size}
main_data %>%
  ggplot(aes(x=Installs,y=Size)) + geom_point() + scale_x_log10()
```
To nobodies surprise, it doesn't seem that there is a relationship between how large an app is and how many installs it receives. Even at the extreme end of installs, the size of an app does not seem to dictate how popular it will be.


One of the questions, I've always wanted to learn was the relationship between installs and rating. For an app, the number of installs is one of the main metrics used to determine success. Rating, is another one of those metrics. In order to see the relationship between these two variables let's plot them. I also scaled the x axis logarithmically in order to make the data easier to visualize.

```{r simple_plot, warning=FALSE}
main_data %>%
  ggplot(aes(x=Installs,y=Rating)) + geom_point() + ylim(0,5) + geom_smooth(method=lm) + scale_x_log10()
```

Due to the discrete nature of our install column, our graph looks like this. Our linear regression model also appears to be relatively flat indicating that there is almost no relationship between the two variables. Let's look at another relationship, the relationship between last updated and installs. This time, we will separate based on popularity and apply a linear regression.

```{r simple plot, warning=FALSE}
main_data %>%
  ggplot(aes(x=`Last Updated`,y=Installs,group=factor(is_popular),colour=factor(is_popular))) + geom_point() + geom_smooth(method=lm)

```

As we can see, the popular apps have a slightly upward trend when it comes to the linear regression. This makes sense as we can expect apps that are popular to be updated more recently than all other apps. But how much? 
Let's take a look at the exact numbers and see what we can gather.

```{r numbers}
pop_linear <- lm(Installs~`Last Updated`,data=popular_apps)
pop_linear
non_pop_linear <- lm(Installs~`Last Updated`,data=unpopular_apps)
non_pop_linear
```
From this linear regression, we can conclude many things about the data we are looking at. Assuming linearity, the intercept for each regression represents how many installs an app would have to start at the very beginning of time which is less important for our uses since we're only talking about a particular time frame starting from around 2010. However, more importantly is the coefficient of the regression which represents how many installs one can expect by updating their app one day later. We can see that for the popular app this coefficient is much larger indicating that a more recent update will lead to a higher increase in installs. 
One additional thing that we can check however, is how other factors play into the success of an app. Last updated date is just one aspect that we can check however, I hypothesize that category can also play an important factor in determining how successful an app can be. In order to do that, we have to add an interaction term for category

```{r interaction}
pop_interaction <- lm(Installs~`Last Updated`*Category,data=popular_apps)
tidy(pop_interaction) %>%
  arrange(desc(estimate))
non_pop_interaction <- lm(Installs~`Last Updated`*Category,data=unpopular_apps)
anova(pop_interaction)
anova(non_pop_interaction)
```
For our statistical analysis, we mainly want to focus on the last column. The smaller the value, the higher the likelihood that our statistical model is likely. As we can see, with the non popular apps (9.027e-11), the interaction term is much more accurate in predicting total installs than with our popular group (0.5945).
Let's plot the residuals for this data set to see what we can find.

```{r interaction_non_pop}
non_pop_augment <- augment(non_pop_interaction) %>%
  ggplot(aes(x=.fitted, y=.resid)) + geom_point()
non_pop_augment
```
This residual plot tells us that the data that we are matching probably does not do well with a linear regression. Despite the fact that the residuals are very small, they don't seem to be distributed around 0 in a random fashion. There definitely appears to be a trend to the residuals.

Our last magic trick of today is to use a geom_smooth line in order to graph the top 10 categories based on mean installs.

```{r something,warning=FALSE}
main_data %>%
  filter(Category %in% c("COMMUNICATION","SOCIAL","VIDEO_PLAYERS","VIDEO","PRODUCTIVITY","GAME","PHOTOGRAPHY","TRAVEL_AND_LOCAL","NEWS_AND_MAGAZINE","ENTERTAINMENT","TOOLS")) %>%
  ggplot(aes(x=`Last Updated`,y=Installs,group=Category,colour=factor(Category)))  + geom_smooth() + geom_point() + scale_y_log10() + facet_wrap(~Category)
```



